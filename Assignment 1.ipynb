{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find the closest words to Alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'V' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-92a61d193894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vectors_co_occurrence.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'V' is not defined"
     ]
    }
   ],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = []\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for cbow\n",
    "\n",
    "def generate_data(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CBOW model\n",
    "dim=50\n",
    "cbow50 = Sequential()\n",
    "cbow50.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow50.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow50.add(Dense(V, activation='softmax'))\n",
    "\n",
    "dim=150\n",
    "cbow150 = Sequential()\n",
    "cbow150.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow150.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow150.add(Dense(V, activation='softmax'))\n",
    "\n",
    "dim=300\n",
    "cbow300 = Sequential()\n",
    "cbow300.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow300.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow300.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function\n",
    "loss='categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41686.856939315796\n",
      "1 39117.07975804806\n",
      "2 38908.93960213661\n"
     ]
    }
   ],
   "source": [
    "#train cbow50 model\n",
    "cbow50.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow50.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "    \n",
    "f = open('cbow_50_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 50))\n",
    "vectors = cbow50.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41624.23085522652\n",
      "1 38758.48308968544\n",
      "2 38354.18330681324\n"
     ]
    }
   ],
   "source": [
    "#train cbow150 model\n",
    "loss='categorical_crossentropy'\n",
    "cbow150.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow150.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "    \n",
    "f = open('cbow_150_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 150))\n",
    "vectors = cbow150.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41559.90414094925\n",
      "1 38492.52104020119\n",
      "2 37942.09509307146\n"
     ]
    }
   ],
   "source": [
    "#train cbow300 model\n",
    "loss='categorical_crossentropy'\n",
    "cbow300.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow300.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('cbow_300_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 300))\n",
    "vectors = cbow300.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(words[i])\n",
    "            if in_words != []:\n",
    "                all_in.append(np.array(in_words,dtype=np.int32))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out)\n",
    "\n",
    "#get x and y's for data\n",
    "x,y = generate_data_skipgram(corpus,window_size,V)\n",
    "\n",
    "#save the preprocessed data of Skipgram\n",
    "f = open('data_skipgram.txt' ,'w')\n",
    "\n",
    "for inputs, outcome in zip(x,y):\n",
    "    inputs = np.concatenate(inputs)\n",
    "    f.write(\" \".join(map(str, list(inputs))))\n",
    "    f.write(\",\")\n",
    "    outcome = np.concatenate(outcome)\n",
    "    f.write(\" \".join(map(str,list(outcome))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the preprocessed Skipgram data\n",
    "def generate_data_skipgram_from_file():\n",
    "    f = open('data_skipgram.txt' ,'r')\n",
    "    for row in f:\n",
    "        inputs,outputs = row.split(\",\")\n",
    "        inputs = np.fromstring(inputs, dtype=int, sep=' ')\n",
    "        inputs = np.asarray(np.split(inputs, len(inputs)))\n",
    "        outputs = np.fromstring(outputs, dtype=float, sep=' ')\n",
    "        outputs = np.asarray(np.split(outputs, len(inputs)))\n",
    "        yield (inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create Skipgram model\n",
    "dim=50\n",
    "skipgram50 = Sequential()\n",
    "skipgram50.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram50.add(Reshape((dim, )))\n",
    "skipgram50.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "dim=150\n",
    "skipgram150 = Sequential()\n",
    "skipgram150.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram150.add(Reshape((dim, )))\n",
    "skipgram150.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "dim=300\n",
    "skipgram300 = Sequential()\n",
    "skipgram300.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram300.add(Reshape((dim, )))\n",
    "skipgram300.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41271.78884887695\n",
      "1 39101.341372966766\n",
      "2 39265.00240945816\n"
     ]
    }
   ],
   "source": [
    "#train Skipgram50 model\n",
    "\n",
    "skipgram50.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram50.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('skipgram_50_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 50))\n",
    "vectors = skipgram50.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41215.38952541351\n",
      "1 38922.67157101631\n",
      "2 38985.02361166477\n"
     ]
    }
   ],
   "source": [
    "#train Skipgram150 model\n",
    "loss='categorical_crossentropy'\n",
    "skipgram150.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram150.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('skipgram_150_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 150))\n",
    "vectors = skipgram150.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41158.67087030411\n",
      "1 38737.37133550644\n",
      "2 38703.309982299805\n"
     ]
    }
   ],
   "source": [
    "#train Skipgram300 model\n",
    "loss='categorical_crossentropy'\n",
    "skipgram300.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram300.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('skipgram_300_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 300))\n",
    "vectors = skipgram300.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create CBOW model with additional dense layer\n",
    "dim=50\n",
    "cbow50h = Sequential()\n",
    "cbow50h.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow50h.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow50h.add(Dense(V, activation='relu'))\n",
    "cbow50h.add(Dense(V, activation='softmax'))\n",
    "\n",
    "dim=150\n",
    "cbow150h = Sequential()\n",
    "cbow150h.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow150h.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow150h.add(Dense(V, activation='relu'))\n",
    "cbow150h.add(Dense(V, activation='softmax'))\n",
    "\n",
    "dim=300\n",
    "cbow300h = Sequential()\n",
    "cbow300h.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow300h.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow300h.add(Dense(V, activation='relu'))\n",
    "cbow300h.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38203.86327829957\n",
      "1 38013.09641942382\n",
      "2 37855.625559709966\n"
     ]
    }
   ],
   "source": [
    "#train model for CBOW + dense\n",
    "loss='categorical_crossentropy'\n",
    "cbow50h.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow50h.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('cbowh_50_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 50))\n",
    "vectors = cbow50h.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 39556.89970886707\n",
      "1 38192.07329599559\n",
      "2 37617.63297378272\n"
     ]
    }
   ],
   "source": [
    "loss='categorical_crossentropy'\n",
    "cbow150h.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow150h.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('cbowh_150_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 150))\n",
    "vectors = cbow150h.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37681.00657823682\n",
      "1 37109.283381693065\n",
      "2 36645.58560779132\n"
     ]
    }
   ],
   "source": [
    "loss='categorical_crossentropy'\n",
    "cbow300h.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow300h.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('cbowh_300_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 300))\n",
    "vectors = cbow300h.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create Skipgram with additional dense layer\n",
    "from keras.layers import Dropout\n",
    "\n",
    "dropout_rate = 0.2\n",
    "dim=50\n",
    "skipgram50h = Sequential()\n",
    "skipgram50h.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram50h.add(Reshape((dim, )))\n",
    "skipgram50h.add(Dense(units=V, activation='relu'))\n",
    "skipgram50h.add(Dense(units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "# dim=50\n",
    "# skipgram50h = Sequential()\n",
    "# skipgram50h.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "# skipgram50h.add(Dense(dim, activation='relu'))\n",
    "# skipgram50h.add(Dense(dim, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "dim=150\n",
    "skipgram150h = Sequential()\n",
    "skipgram150h.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram150h.add(Reshape((dim, )))\n",
    "skipgram150h.add(Dense(units=V, activation='relu'))\n",
    "skipgram150h.add(Dense(units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "dim=300\n",
    "skipgram300h = Sequential()\n",
    "skipgram300h.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram300h.add(Reshape((dim, )))\n",
    "skipgram300h.add(Dense(units=V, activation='relu'))\n",
    "skipgram300h.add(Dense(units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38556.30892801285\n",
      "1 38284.277649879456\n",
      "2 38160.48908627033\n"
     ]
    }
   ],
   "source": [
    "#train model for Skipgram + dense\n",
    "skipgram50h.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram50h.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('skipgramh_50_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 50))\n",
    "vectors = skipgram50h.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38524.81856560707\n",
      "1 38182.9237254858\n",
      "2 38044.449105620384\n"
     ]
    }
   ],
   "source": [
    "loss='categorical_crossentropy'\n",
    "skipgram150h.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram150h.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('skipgramh_150_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 150))\n",
    "vectors = skipgram150h.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38556.01204943657\n",
      "1 38273.90782046318\n",
      "2 38062.221319675446\n"
     ]
    }
   ],
   "source": [
    "loss='categorical_crossentropy'\n",
    "skipgram300h.compile(loss=loss, optimizer='adadelta')\n",
    "\n",
    "for ite in range(3):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram300h.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('skipgramh_300_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, 300))\n",
    "vectors = skipgram300h.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4399814051504332 0.6700274476836526 1.0137254515416014\n",
      "0.45211010872788465 0.7508679510302093 1.1153392875008725\n",
      "0.539148724970009 0.9515944692008453 1.245625942920956\n",
      "0.549228284260227 0.9984298874741127 1.259030344350032\n"
     ]
    }
   ],
   "source": [
    "#Implement your own analogy function\n",
    "\n",
    "from math import *\n",
    "\n",
    "#options: cbow_50, cbow_150, cbow_300, cbowh_50, cbowh_150, cbowh_300\n",
    "# skipgram_50, skipgram_150, skipgram_300, skipgramh_50, skipgramh_150, skipgramh_300\n",
    "def measure_analogy(word1, word2, word3, word4, db):\n",
    "    filename = ''+db+'_vectors.txt'\n",
    "    vectorDict = {}\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        word, vector = line.partition(\" \")[::2]\n",
    "        vectorDict[word.strip()] = [float(v) for v in vector.split(\" \")]\n",
    "    vector1 = vectorDict[word1]\n",
    "    vector2 = vectorDict[word2]\n",
    "    vector3 = vectorDict[word3]\n",
    "    vector4 = vectorDict[word4]\n",
    "    #Calculate v1 -v2 + v4 to compare to v3\n",
    "    lhs_intermed = map(float.__sub__, vector1, vector2)\n",
    "    lhs = list(map(float.__add__, lhs_intermed, vector4))\n",
    "    rhs = vector3\n",
    "    #Calculate Euclidian distance between the two sides of the equation\n",
    "    dist = sqrt(sum(pow(a-b,2) for a, b in zip(lhs, rhs)))\n",
    "    return dist\n",
    "\n",
    "cbow50_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"cbow_50\")\n",
    "cbow150_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"cbow_150\")\n",
    "cbow300_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"cbow_300\")\n",
    "print(cbow50_similarity, cbow150_similarity, cbow300_similarity)\n",
    "\n",
    "cbow50h_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"cbowh_50\")\n",
    "cbow150h_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"cbowh_150\")\n",
    "cbow300h_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"cbowh_300\")\n",
    "print(cbow50h_similarity, cbow150h_similarity, cbow300h_similarity)\n",
    "\n",
    "skipgram50_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"skipgram_50\")\n",
    "skipgram150_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"skipgram_150\")\n",
    "skipgram300_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"skipgram_300\")\n",
    "print(skipgram50_similarity, skipgram150_similarity, skipgram300_similarity)\n",
    "\n",
    "skipgram50h_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"skipgramh_50\")\n",
    "skipgram150h_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"skipgramh_150\")\n",
    "skipgram300h_similarity = measure_analogy(\"girl\", \"child\", \"circle\", \"shape\", \"skipgramh_300\")\n",
    "print(skipgram50h_similarity, skipgram150h_similarity, skipgram300h_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4399814051504332, 0.6700274476836526, 1.0137254515416014], [0.45211010872788465, 0.7508679510302093, 1.1153392875008725], [0.539148724970009, 0.9515944692008453, 1.245625942920956], [0.549228284260227, 0.9984298874741127, 1.259030344350032]]\n"
     ]
    }
   ],
   "source": [
    "#Create a 2D array of the results of the previous question\n",
    "row1 = [cbow50_similarity, cbow150_similarity, cbow300_similarity]\n",
    "row2 = [cbow50h_similarity, cbow150h_similarity, cbow300h_similarity]\n",
    "row3 = [skipgram50_similarity, skipgram150_similarity, skipgram300_similarity]\n",
    "row4 = [skipgram50h_similarity, skipgram150h_similarity, skipgram300h_similarity]\n",
    "\n",
    "mappingArray = []\n",
    "mappingArray.append(row1)\n",
    "mappingArray.append(row2)\n",
    "mappingArray.append(row3)\n",
    "mappingArray.append(row4)\n",
    "print(mappingArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD3CAYAAAAjdY4DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5P8BCXCFkZVz/bAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a360208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualization results trained word embeddings\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(mappingArray,  annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Interpretation results of the visualization\n",
    "\n",
    "As the analogy function that we created is based on Euclidian distance, the lower the value, the better the semantic relationships have been conserved. From the heatmap we can see that the distances are the lowest for CBOW without extra hidden layer. The relationships within a model between any two embedding sizes seem to be almost linear, regardless of the model you are looking at. \n",
    "\n",
    "Another thing that jumps out is that the Skipgram models of embedding vector size 50 seem to find similar values to the CBOW models with embedding size 150. A similar effect is seen between CBOW 300 and Skipgram 150. \n",
    "\n",
    "It is important to note however, that all distances seem relatively small anyway. In fact, the characteristic that some models find higher distances does not mean that these models are less accurate. It may well be the case that the \"true\" analogy value of the analogy chosen in the previous question is closer to a value of 1, as found by the models with larger embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Compare the results of the trained word embeddings with the word-word co-occurrence matrix\n",
    "\n",
    "Code can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-fb43d3f3a300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_word2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"your path /GoogleNews-vectors-negative300.bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/daphne/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1002\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daphne/anaconda/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"your path /GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
