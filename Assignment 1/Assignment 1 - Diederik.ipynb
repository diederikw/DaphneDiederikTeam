{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\2imm10\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create co-occurrence matrix\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "coMatrix=np.zeros((V, V), np.int8)\n",
    "\n",
    "def processSentence(sentence):\n",
    "    # Run over the sentence from all words as a co-occurrence\n",
    "    for w1 in range(0, len(sentence)):\n",
    "        word1 = sentence[w1]\n",
    "        \n",
    "        # Set window size\n",
    "        window = []\n",
    "        for x in range(1, window_size_corpus+1):\n",
    "            window.append(w1+x)\n",
    "            window.append(w1-x)\n",
    "        \n",
    "        # Compare w1 to every w2 in the window size \n",
    "        for w2 in window:\n",
    "            if w2 >= 0 and w2 < len(sentence):\n",
    "                word2 = sentence[w2]\n",
    "                if(word1 != word2):\n",
    "                    coMatrix[word1][word2] += 1\n",
    "\n",
    "for sentence in corpus:\n",
    "    processSentence(sentence)\n",
    "\n",
    "# Normalize data to  be a percentage (value between 0 and 1, depending on the maximum value in the entire matrix).\n",
    "divMatrix = np.zeros((V,V))\n",
    "divMatrix.fill(coMatrix.max())\n",
    "coMatrix = np.divide(coMatrix, divMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between Alice and Rabbit:\n",
      "[[0.47890931]]\n",
      "\n",
      "Cosine similarity between Alice and Dinah:\n",
      "[[0.39360011]]\n",
      "\n",
      "Cosine similarity between Dinah and Rabbit:\n",
      "[[0.29862324]]\n"
     ]
    }
   ],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit\n",
    "dictionary = tokenizer.word_index\n",
    "alice = dictionary['alice']\n",
    "rabbit = dictionary['rabbit']\n",
    "dinah = dictionary['dinah']\n",
    "\n",
    "print(\"Cosine similarity between Alice and Rabbit:\")\n",
    "print(cosine_similarity(coMatrix[alice].reshape(1, -1), coMatrix[rabbit].reshape(1, -1)))\n",
    "print(\"\\nCosine similarity between Alice and Dinah:\")\n",
    "print(cosine_similarity(coMatrix[alice].reshape(1, -1), coMatrix[dinah].reshape(1, -1)))\n",
    "print(\"\\nCosine similarity between Dinah and Rabbit:\")\n",
    "print(cosine_similarity(coMatrix[dinah].reshape(1, -1), coMatrix[rabbit].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\n",
      "her\n",
      "that\n",
      "herself\n",
      "for\n",
      "on\n"
     ]
    }
   ],
   "source": [
    "#find the 5 closest words to Alice\n",
    "\n",
    "# Had to reshape data, but then I got an error that 1183 is not in the list. 1183 shouldn't be in the list, so why does\n",
    "# it want to check it?\n",
    "# This only happens with coMatrixPerc. TODO\n",
    "# Reshape your data either using array.reshape(-1, 1) if your data has a single feature or \n",
    "# array.reshape(1, -1) if it contains a single sample.\n",
    "\n",
    "#Find nearest neighbors\n",
    "neighbors = nn(n_neighbors=6)\n",
    "neighbors.fit(coMatrix) \n",
    "similars = (neighbors.kneighbors(coMatrix[alice].reshape(1, -1))[1])[0]\n",
    "\n",
    "#translate ids back into words:\n",
    "for word in similars:\n",
    "    print(list(dictionary.keys())[list(dictionary.values()).index(word)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of the five closest words to Alice:\n",
    "The five closest words contain in our opinion two general groups. The so referred to as 'filler' words, containing the words \"that\", \"for\", and \"on\", and the 'referal' words, containing the words \"her\", and \"herself\". \n",
    "\n",
    "The filler words contain words which are very common in the English language and do not have any semantic meaning. The corpus contained the entirety of the 'Alice in Wonderland' book, including words such as that, for, on, the, a, etc., which means that these common words will be more present in the book. Especially when comparing it to words with semantic meaning, which will not be present in almost every sentence as opposed to such filler words. It is not a surprise that these filler words are among the closest neighbors to Alice, but unfortunately there's not a lot which can be learned from their presence. It is probably better to eliminate filler words such as these from the corpus before performing comparison tasks such as these. \n",
    "\n",
    "The referal words contain the words her, and herself. These words are also very common in the English language, but are slightly different as they are used to refer to a person or used as a possesive indication. Both her, and herself are used to refer to female people, which in this case will most often have been used to refer to Alice, or items belonging to Alice. Not much can be learned from this, apart from gender. In other cases, it is probably better to elimate these kinds of words from the corpus as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of the drawbacks of a term-term co-occurrence matrix solutions:\n",
    "\n",
    "The main drawbacks of a term-term co-occurence matrix are that the matrices will be sparse, the matrices will be very large, and it is not very usefull for many tasks. \n",
    "\n",
    "The matrix of a term-term co-occurrence matrix solution will be very sparce, in other words, it will contain a lot of 0's compared to the amount of usefull data (usfull data is anything between 0 and 1 NOT including 0 in our case). This useless data will be a drain on a system as its operations and memory are wasted on the zeros, especially if standard operations made for dense matrices are used. It also makes it difficult for someone to visually inspect and explore the data during analysis, requiring them to completely rely on the operations done by the computer.\n",
    "\n",
    "Due to the large amount of words in any usefull corpus, the matrix of this co-occurrence matrix solutions will also be very large. This will result in requiring a lot of memory for the matrix, difficulty with visually inspecting and exploring the data, and making it necessary to rely on operations and algorithms to process the data correct.\n",
    "\n",
    "A term-term co-occurrence matrix solution is usefull for comparing words, but is not very usefull for a general inspection or exploration of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "\n",
    "# Write all the words in the file\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = coMatrix[:,1:]\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "    ReLu Standaard, meestal best presterend. SoftMax omdat het er al inzat. \n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for cbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create CBOW model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create Skipgram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for Skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train Skipgram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create CBOW model with additional dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create Skipgram with additional dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model for Skipgram + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualization results trained word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram\n",
    "\n",
    "**Advantages of CBOW and Skipgram:** \n",
    "\n",
    "CBOW and Skipgram have a number of advantages. CBOW and Skipgram have a lower internal complexity compared to older models, which means that it can be trained on a lot more data much more efficiently. The models also have the ability to complete more complex similarity tasks, e.g. being able to denote a similarity between *big*, *bigger* and *small*, *smaller*. The models also are able to represent very subtle semantic relationships, compared to other models. \n",
    "\n",
    "**Advantages of negative sampling:**\n",
    "\n",
    "A disadvantage of CBOW and Skipgram with word processing is the hugh amount of weights which need to be updated every single run while training. Negative sampling allows more efficient training of a model, due to the significantly reduced amount of weights which are updated every run. \n",
    "\n",
    "If Subsampling was applied to frequent words, before applying negative sampling, not only reduces the computational burden of training, but also improves the quality of the resulting word vectors.\n",
    "\n",
    "**Drawbacks of CBOW and Skipgram:**\n",
    "\n",
    "One drawback of CBOW and Skipgram is that words with multiple meaning can only be interpreted once. Which means it will only interpet all meanings at the same time, which would not provide a good representation for words with multiple meanings.\n",
    "\n",
    "Another drawback of CBOW and Skipgram is that the models only look at words and the window around it, but not to the order of the words within the window. The order of words can be important as meanings can change depending on it, such as *race horse*, and *horse race*. Two words which have a different semantic meaning, but will be treated as the same thing by CBOW and Skipgram.\n",
    "\n",
    "Depending on the size of the corpus, there can be an extremely large number of weights to be altered during trainnig, especially if dropout or negative sampling is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"your path /GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
