{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDlRxdcd2GxS"
   },
   "source": [
    "# Assignment 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8DoqePg2GxU"
   },
   "source": [
    "# Image Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Ejzn1Hw2GxW"
   },
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pf3_XFD2GxX"
   },
   "source": [
    "We will use Microsoft COCO (Common Objects in Context) data set to train our \"Image Caption Retrieval Model\". This data set consists of pretrained 10-crop VGG19 features (Neural codes) and its corresponding text caption. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QQq8_IHQ2GxZ"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "#DATA_PATH = 'img_cap_coco' #(If Google colab)\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embeddings'\n",
    "MODEL_PATH = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvRXoIG32Gyk"
   },
   "source": [
    "You will need to create above directories and locate data set provided in directory 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91125,
     "status": "ok",
     "timestamp": 1523710524999,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "EkI4FSYxB2Ta",
    "outputId": "615f903d-7470-4111-9669-169abcb57ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-04-14 12:53:59--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2018-04-14 12:53:59--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip         92%[=================>  ] 756.93M  14.6MB/s    eta 3s     glove.6B.zip        100%[===================>] 822.24M  14.6MB/s    in 35s     \n",
      "\n",
      "2018-04-14 12:54:34 (23.7 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "--2018-04-14 12:54:35--  https://storage.googleapis.com/trl_data/img_cap_coco.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 251061503 (239M) [application/zip]\n",
      "Saving to: ‘img_cap_coco.zip’\n",
      "\n",
      "img_cap_coco.zip    100%[===================>] 239.43M  71.2MB/s    in 3.4s    \n",
      "\n",
      "2018-04-14 12:54:39 (71.2 MB/s) - ‘img_cap_coco.zip’ saved [251061503/251061503]\n",
      "\n",
      "--2018-04-14 12:54:40--  http://images.cocodataset.org/zips/val2014.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|74.125.141.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6645013297 (6.2G) [application/zip]\n",
      "Saving to: ‘val2014.zip’\n",
      "\n",
      "val2014.zip          33%[=====>              ]   2.05G   132MB/s    eta 30s    val2014.zip         100%[===================>]   6.19G   145MB/s    in 44s     \n",
      "\n",
      "2018-04-14 12:55:24 (143 MB/s) - ‘val2014.zip’ saved [6645013297/6645013297]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!wget https://storage.googleapis.com/trl_data/img_cap_coco.zip\n",
    "!wget http://images.cocodataset.org/zips/val2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5855,
     "status": "ok",
     "timestamp": 1523711870387,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "TkcwlE7mEATU",
    "outputId": "1cb22d22-3683-4c69-c00b-a3ae4741a10f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab  glove6B  img_cap_coco\timg_cap_coco.zip  val2014  val2014.zip\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir glove6B\n",
    "!mv glove.6B.zip glove6B/\n",
    "\n",
    "!unzip glove6B/glove.6B;\n",
    "!unzip img_cap_coco;\n",
    "!unzip val2014;\n",
    "\n",
    "!mv glove.6B.100d.txt glove6B/\n",
    "!mv glove.6B.50d.txt glove6B/\n",
    "!mv glove.6B.200d.txt glove6B/\n",
    "!mv glove.6B.300d.txt glove6B/\n",
    "\n",
    "!ls\n",
    "\n",
    "#rm -r foldername\n",
    "#rm filename\n",
    "#mv oldfoldername newfoldername"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4017,
     "status": "ok",
     "timestamp": 1523718333880,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "JDWC1crysBtG",
    "outputId": "a9fa6fe4-2a47-4aa3-ef60-dd31885b2828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab     glove6B\t  img_cap_coco.zip  val2014\r\n",
      "embeddings  img_cap_coco  models\t    val2014.zip\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir embeddings\n",
    "!mkdir models\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2C5Fp8yy2Gyl"
   },
   "source": [
    "#### Reading pairs of image (VGG19 features) - caption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "d4vdcyv_2Gyn"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "import collections\n",
    "\n",
    "np_train_data = np.load(os.path.join(DATA_PATH,'train_data.npy'))\n",
    "np_val_data = np.load(os.path.join(DATA_PATH,'val_data.npy'))\n",
    "\n",
    "train_data = collections.OrderedDict()\n",
    "for i in range(len(np_train_data.item())):\n",
    "    cap =  np_train_data.item()['caps']\n",
    "    img =  np_train_data.item()['ims']\n",
    "    train_data['caps'] = cap\n",
    "    train_data['ims'] = img\n",
    "    \n",
    "val_data = collections.OrderedDict()\n",
    "for i in range(len(np_val_data.item())):\n",
    "    cap =  np_val_data.item()['caps']\n",
    "    img =  np_val_data.item()['ims']\n",
    "    val_data['caps'] = cap\n",
    "    val_data['ims'] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1523721732166,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "daBqDrc12Gys",
    "outputId": "fa91458f-7ebf-4b33-e3ef-467fa48400f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'a woman is working in a kitchen carrying a soft toy'"
      ]
     },
     "execution_count": 208,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of caption\n",
    "train_data['caps'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1523721732944,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "MwT4pakb2Gy2",
    "outputId": "286d78a7-1d71-4214-d7ba-0dd2c34be355"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02239205, 0.00604904, 0.02322354, ..., 0.        , 0.00106503,\n",
       "       0.00711824], dtype=float32)"
      ]
     },
     "execution_count": 209,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of pre-computed VGG19 features\n",
    "val_data['ims'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnOQ8-h42GzV"
   },
   "source": [
    "#### Reading caption and information about its corresponding raw images from Microsoft COCO website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1ThT3lKP2Gz3"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "# use them for your own additional preprocessing step\n",
    "# to map precomputed features and location of raw images \n",
    "\n",
    "import json\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'instances_val2014.json')) as json_file:\n",
    "    coco_instances_val = json.load(json_file)\n",
    "    \n",
    "with open(os.path.join(DATA_PATH,'captions_val2014.json')) as json_file:\n",
    "    coco_caption_val = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjvuSCdv2G1M"
   },
   "source": [
    "#### Additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1523721742087,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "LRaRx26Z2G1Q",
    "outputId": "f5e2e81b-2054-4e4a-d3cc-4d8ebfbca78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://farm8.staticflickr.com/7396/8750681361_391310447e_z.jpg', array([0.        , 0.        , 0.00112501, ..., 0.        , 0.        ,\n",
      "       0.        ], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# create your own function to map pairs of precomputed features and filepath of raw images\n",
    "# this will be used later for visualization part\n",
    "# simple approach: based on matched text caption (see json file)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "def return_imagepair(number):\n",
    "    img = coco_instances_val['images'][number]['flickr_url']\n",
    "    features = val_data['ims'][number]\n",
    "    return(img, features)\n",
    "\n",
    "print(return_imagepair(4999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2PpXxzJ2G3x"
   },
   "source": [
    "#### Build vocabulary index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 643,
     "status": "ok",
     "timestamp": 1523721742855,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "Y5ew5lqD2G3y",
    "outputId": "69ee7d09-87ff-485b-b12b-8c01e4c8b74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 11473\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "def build_dictionary(text):\n",
    "\n",
    "    wordcount = OrderedDict()\n",
    "    for cc in text:\n",
    "        words = cc.split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 0\n",
    "            wordcount[w] += 1\n",
    "    words = list(wordcount.keys())\n",
    "    freqs = list(wordcount.values())\n",
    "    sorted_idx = np.argsort(freqs)[::-1]\n",
    "    \n",
    "\n",
    "    worddict = OrderedDict()\n",
    "    worddict['<pad>'] = 0\n",
    "    worddict['<unk>'] = 1\n",
    "    for idx, sidx in enumerate(sorted_idx):\n",
    "        worddict[words[sidx]] = idx+2  # 0: <pad>, 1: <unk>\n",
    "    \n",
    "\n",
    "    return worddict\n",
    "\n",
    "# use the resulting vocabulary index as your look up dictionary\n",
    "# to transform raw text into integer sequences\n",
    "\n",
    "all_captions = []\n",
    "all_captions = train_data['caps'] + val_data['caps']\n",
    "\n",
    "# decode bytes to string format\n",
    "caps = []\n",
    "for w in all_captions:\n",
    "    caps.append(w.decode())\n",
    "    \n",
    "words_indices = build_dictionary(caps)\n",
    "print ('Dictionary size: ' + str(len(words_indices)))\n",
    "indices_words = dict((v,k) for (k,v) in words_indices.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhpjCdqP2G36"
   },
   "source": [
    "### 2. Image - Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UKzhDhm2G4G"
   },
   "source": [
    "### Image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OQvE2jRs2G4I"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "inputs_image = Input(shape=(4096,))\n",
    "image_dense = Dense(1024, activation='relu')(inputs_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elPI2CHX2G4N"
   },
   "source": [
    "### Caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13682,
     "status": "ok",
     "timestamp": 1523721757137,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "k76A4T8Q2G4P",
    "outputId": "6cf34122-a101-4a48-8d23-68543d17d409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# For embedding layer, initialize with pretrained word embedding (GloVe)\n",
    "\n",
    "# Set up the glove embedding\n",
    "GLOVE_DIR = 'glove6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ddpbq-te2G4a"
   },
   "outputs": [],
   "source": [
    "# Transform all captions into integer sequences for the NN\n",
    "#words_indices['rowboat']\n",
    "#embeddings_index['rowboat']\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(caps)\n",
    "sequences = tokenizer.texts_to_sequences(caps)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4TqiHvVz2G45"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_caps = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dKP7klCb2G5G"
   },
   "outputs": [],
   "source": [
    "# Create the embedding matrix\n",
    "from numpy import zeros\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "F8Z356dc2G5K"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Input, Dense, Embedding, Reshape, GRU, merge \n",
    "from keras.layers import LSTM, Dropout, BatchNormalization, Activation, TimeDistributed, dot\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Create the caption model\n",
    "#TODO: inputs_caption = Input(shape=(15,))\n",
    "inputs_caption = Input(shape=(50,))\n",
    "embed = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputs_caption)\n",
    "lstm = LSTM(256, return_sequences=True)(embed)\n",
    "dense = TimeDistributed(Dense(256, activation='relu'))(lstm)\n",
    "flattened = Flatten()(dense)\n",
    "caption_dense = Dense(1024, activation='relu')(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rSbJQzp2G5P"
   },
   "source": [
    "### Join model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GXtGgJk42G5Q"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "dotproduct = dot([image_dense, caption_dense], axes=-1)\n",
    "# layer for computing dot product between tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMuP1Hzq2G5r"
   },
   "source": [
    "### Main model for training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1523721763232,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "EWqn51iv2HBE",
    "outputId": "a5b710da-acd4-44c7-d8c7-e6c9b462a60c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training model\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "print (\"loading the training model\")\n",
    "training_model = Model(inputs=[inputs_image, inputs_caption], outputs=[dotproduct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDFCPzDz2HBz"
   },
   "source": [
    "### Retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1523721764114,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "o6Ctw6Eq2HB0",
    "outputId": "2224a452-2df6-4a01-afcd-5b09735987f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sub-models for retrieving Neural codes\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "print (\"loading sub-models for retrieving Neural codes\")\n",
    "caption_model = Model(inputs=inputs_caption, outputs=caption_dense)\n",
    "image_model = Model(inputs=inputs_image, outputs=image_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wGo3EEu2HB9"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQv-ASls2HB-"
   },
   "source": [
    "We define our loss function as a loss for maximizing the margin between a positive and\n",
    "negative example.  If we call $p_i$ the score of the positive pair of the $i$-th example, and $n_i$ the score of the negative pair of that example, the loss is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clNYMz-c2HB_"
   },
   "source": [
    "\\begin{equation*}\n",
    "loss = \\sum_i{max(0, 1 -p_i + n_i)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "onSiEfoZ2HCB"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def max_margin_loss(y_true, y_pred):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    loss_ = K.sum(K.maximum(0.0, 1.0 - y_pred[0] + y_pred[1]))\n",
    "    \n",
    "    return loss_\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lPjHGzc2HCK"
   },
   "source": [
    "#### Accuracy metric for max-margin loss\n",
    "How many times did the positive pair effectively get a higher value than the negative pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ponLLetO2HCL"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    accuracy_ = K.mean(y_pred[0] > y_pred[1])\n",
    "    \n",
    "    return accuracy_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcXDuKUy2HCS"
   },
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1523721766362,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "5QEoFIig2HCT",
    "outputId": "6cbec226-b614-42c0-871a-cbe1401b5a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the training model\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "print (\"compiling the training model\")\n",
    "training_model.compile(optimizer='adam', loss=max_margin_loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDYlRPFm2HDF"
   },
   "source": [
    "### 3. Data preparation for training the model\n",
    "\n",
    "* adjust the length of captions into fixed maximum length (50 words)\n",
    "* sampling caption for each image, while shuffling the image data\n",
    "* encode captions into integer format based on look-up vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jn9lFTBi2HDJ"
   },
   "outputs": [],
   "source": [
    "# sampling one caption per image\n",
    "# return image_ids, caption_ids\n",
    "from random import random\n",
    "from math import floor\n",
    "\n",
    "def sampling_img_cap(data):\n",
    "    \n",
    "    ims = data['ims']\n",
    "    image_ids = []\n",
    "    caption_ids = []\n",
    "    for ids in range(0, len(ims)):\n",
    "        # Random number between 1 and 5 for each img\n",
    "        i = floor(random() * 5)\n",
    "        caption = ids * 5 + i\n",
    "        image_ids.append(ids)\n",
    "        caption_ids.append(caption)\n",
    "    \n",
    "    image_ids = np.array(image_ids)\n",
    "    caption_ids = np.array(caption_ids)\n",
    "    return image_ids, caption_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HKJiqwhh2HDZ"
   },
   "outputs": [],
   "source": [
    "# transform raw text caption into integer sequences of fixed maximum length\n",
    "\n",
    "def prepare_caption(caption_ids, caption_data):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # did this already above\n",
    "    caption_seqs = []\n",
    "    \n",
    "    for i in caption_ids:\n",
    "        if (len(caption_data) == 50000):\n",
    "            caption_seqs.append(padded_caps[i])\n",
    "        else:\n",
    "            caption_seqs.append(padded_caps[i+50000])\n",
    "       \n",
    "    return np.stack(caption_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1QDfY_zL2HDl"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_caps = []\n",
    "for cap in train_data['caps']:\n",
    "    train_caps.append(cap.decode())\n",
    "\n",
    "val_caps = []\n",
    "for cap in val_data['caps']:\n",
    "    val_caps.append(cap.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mEFX8OLz2HDp"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_image_ids, train_caption_ids = sampling_img_cap(train_data)\n",
    "val_image_ids, val_caption_ids = sampling_img_cap(val_data)\n",
    "\n",
    "x_caption = prepare_caption(train_caption_ids, train_caps)\n",
    "x_image = train_data['ims'][np.array(train_image_ids)]\n",
    "\n",
    "x_val_caption = prepare_caption(val_caption_ids, val_caps)\n",
    "x_val_image = val_data['ims'][np.array(val_image_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1Wai1Yj2HDv"
   },
   "source": [
    "### 4. Create noise set for negative examples of image-fake caption and dummy output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qmkVph32HDw"
   },
   "source": [
    "Notice that we do not have real output with labels for training the model. Keras architecture expects labels, so we need to create dummy output -- which is numpy array of zeros. This dummy labels or output is never used since we compute loss function based on margin between positive examples (image-real caption) and negative examples (image-fake caption)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Vpmoikg52HDy"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def create_noise_caption_ids(image_ids):\n",
    "    max_caption_id= (len(image_ids)-1)*5+4\n",
    "    \n",
    "    caption_ids = []\n",
    "    i = 0\n",
    "    \n",
    "    while i in range(len(image_ids)-1):\n",
    "        _id = np.random.randint(0, max_caption_id)\n",
    "        if (_id >= (i*5) and _id <= (i*5+4)):\n",
    "            continue\n",
    "        else:\n",
    "            caption_ids.append(_id)\n",
    "            i = i + 1\n",
    "            \n",
    "    return np.stack(caption_ids)\n",
    "\n",
    "train_noise_caption_ids = create_noise_caption_ids(image_ids=train_image_ids)\n",
    "val_noise_caption_ids = create_noise_caption_ids(image_ids=val_image_ids)\n",
    "\n",
    "train_noise = prepare_caption(train_noise_caption_ids, train_caps)\n",
    "val_noise = prepare_caption(val_noise_caption_ids, val_caps)\n",
    "\n",
    "y_train_labels = np.zeros((10000,), dtype=int)\n",
    "y_val_labels = np.zeros((5000,), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llsDt3WL2HD5"
   },
   "source": [
    "### 5. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HlXPyvzz2HD6"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "X_train = [x_image, x_caption]\n",
    "Y_train = y_train_labels\n",
    "X_valid = [x_val_image, x_val_caption]\n",
    "Y_valid = y_val_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2750
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11483,
     "status": "error",
     "timestamp": 1523721781555,
     "user": {
      "displayName": "Diederik W",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110096029821254087179"
     },
     "user_tz": -120
    },
    "id": "uhXwRAqD2HFe",
    "outputId": "347ed0e7-d9b9-4e68-a3d3-8f2d43cb0c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [4096,1024] and type float\n\t [[Node: training_1/Adam/zeros_5 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [4096,1024] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-8435593ecc01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               validation_data=(X_valid, Y_valid))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2474\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2476\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2478\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [4096,1024] and type float\n\t [[Node: training_1/Adam/zeros_5 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [4096,1024] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training_1/Adam/zeros_5', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-231-8435593ecc01>\", line 7, in <module>\n    validation_data=(X_valid, Y_valid))\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1682, in fit\n    self._make_train_function()\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\", line 457, in get_updates\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\", line 457, in <listcomp>\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 692, in zeros\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 1570, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1713, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [4096,1024] and type float\n\t [[Node: training_1/Adam/zeros_5 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [4096,1024] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# fit the model on training and validation set\n",
    "batch_size = 250\n",
    "epochs = 10\n",
    "\n",
    "training_model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYGq0c6h2HFi"
   },
   "source": [
    "#### Storing models and weight parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "REUn2kbO2HFk"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "# Save model\n",
    "training_model.save(os.path.join(MODEL_PATH,'image_caption_model.h5'))\n",
    "# Save weight parameters\n",
    "training_model.save_weights(os.path.join(MODEL_PATH, 'weights_image_caption.hdf5'))\n",
    "\n",
    "# Save model for encoding caption and image\n",
    "caption_model.save(os.path.join(MODEL_PATH,'caption_model.h5'))\n",
    "image_model.save(os.path.join(MODEL_PATH,'image_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ndp676Ii2HFo"
   },
   "source": [
    "### 6. Feature extraction (Neural codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "baFLxgIC2HFp"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Use caption_model and image_model to produce \"Neural codes\" \n",
    "# for both image and caption from validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8svO34792HFv"
   },
   "source": [
    "### 7. Caption Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0Cqz2tH2HFw"
   },
   "source": [
    "#### Display original image as query and its ground truth caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "crm-Mj9o2HFx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IkYCKBAc2HF2"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# choose one image_id from validation set\n",
    "# use this id to get filepath of image\n",
    "img_id = \n",
    "filepath_image = \n",
    "\n",
    "# display original caption\n",
    "original_caption = \n",
    "print(original_caption)\n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "img = image.load_img(os.path.join(IMAGE_DATA,filepath_image), target_size=(224,224))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "id3vV3G-2HF6"
   },
   "outputs": [],
   "source": [
    "# function to retrieve caption, given an image query\n",
    "\n",
    "def get_caption(image_filename, n=10):   \n",
    "    \n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "So0esvrN2HG8"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "get_caption(filepath_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEOHIwSo2HIN"
   },
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "=== write your answer here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87EzGGhw2HIP"
   },
   "source": [
    "### 8. Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "26k4R13h2HIQ"
   },
   "outputs": [],
   "source": [
    "# given text query, display retrieved image, similarity score, and its original caption \n",
    "\n",
    "def search_image(text_caption, n=10):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cjX6ky_2HIX"
   },
   "source": [
    "Consider to use the following settings for image retrieval task.\n",
    "\n",
    "* use real caption that is available in validation set as a query.\n",
    "* use part of caption as query. For instance, instead of use the whole text sentence of the\n",
    "caption, you may consider to use key phrase or combination of words that is included in\n",
    "corresponding caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "F7Y9ls7v2HIZ"
   },
   "outputs": [],
   "source": [
    "# Example of text query \n",
    "# text = 'two giraffes standing near trees'\n",
    "\n",
    "# YOUR QUERY-1\n",
    "text1 = \n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MbW10iY12HId"
   },
   "outputs": [],
   "source": [
    "# YOUR QUERY-2\n",
    "text2 = \n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGBnenBD2HKI"
   },
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "=== write your answer here ==="
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_0Cqz2tH2HFw",
    "uEOHIwSo2HIN",
    "SGBnenBD2HKI"
   ],
   "default_view": {},
   "name": "Assignment_3.3 NEW.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
